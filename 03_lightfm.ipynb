{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xnhGbwevBqRo"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WZQjUfMxNnxh"
   },
   "outputs": [],
   "source": [
    "!pip install pymorphy2 sparse_dot_topn lightfm\n",
    "!python -m nltk.downloader stopwords\n",
    "!python -m nltk.downloader wordnet\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bVC0QR09LGOI"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import json\n",
    "import gzip\n",
    "import codecs\n",
    "\n",
    "from itertools import islice, chain, filterfalse\n",
    "from collections import Counter, defaultdict\n",
    "from operator import itemgetter\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "\n",
    "import joblib\n",
    "\n",
    "import lxml.html as lhtml\n",
    "\n",
    "# from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fkkEzd9lLG20"
   },
   "outputs": [],
   "source": [
    "WORKDIR = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_jTbp3f3B1k5"
   },
   "outputs": [],
   "source": [
    "!mkdir -p \"{WORKDIR}/data\" \"{WORKDIR}/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Li3j0B5qB4d6"
   },
   "outputs": [],
   "source": [
    "def save_array(a, filename: str, sparse: bool = False, **params):\n",
    "    if sparse and not sp.issparse(a):\n",
    "        a = sp.csr_matrix(a)\n",
    "    elif not sparse and sp.issparse(a):\n",
    "        a = np.asarray(a.todense())\n",
    "\n",
    "    with open(filename, 'wb') as f_data:\n",
    "        save = sp.save_npz if sparse else np.save\n",
    "        return save(f_data, a, **params)\n",
    "\n",
    "\n",
    "def load_array(filename: str, sparse: bool = False, **params):\n",
    "    with open(filename, 'rb') as f_data:\n",
    "        load = sp.load_npz if sparse else np.load\n",
    "        return load(f_data, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "j_74JYgdUYYw",
    "outputId": "940f086c-f6f0-4f82-c021-8cbf6299417e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/poisk/.conda/envs/ml-py37/lib/python3.7/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>specializations</th>\n",
       "      <th>is_train</th>\n",
       "      <th>area_id</th>\n",
       "      <th>compensation_from</th>\n",
       "      <th>compensation_to</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>currency</th>\n",
       "      <th>employer</th>\n",
       "      <th>employment</th>\n",
       "      <th>work_experience</th>\n",
       "      <th>work_schedule</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vacancy_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[242, 256, 302, 324, 358, 440]</td>\n",
       "      <td>True</td>\n",
       "      <td>26</td>\n",
       "      <td>22000.0</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>2019-01-24</td>\n",
       "      <td>RUR</td>\n",
       "      <td>0ce23382345c</td>\n",
       "      <td>full</td>\n",
       "      <td>between1And3</td>\n",
       "      <td>fullDay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-07-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b9aa259f8724</td>\n",
       "      <td>full</td>\n",
       "      <td>between1And3</td>\n",
       "      <td>fullDay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[211]</td>\n",
       "      <td>True</td>\n",
       "      <td>1002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11ecc72a7a76</td>\n",
       "      <td>project</td>\n",
       "      <td>between1And3</td>\n",
       "      <td>fullDay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[389, 412, 437]</td>\n",
       "      <td>True</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36000.0</td>\n",
       "      <td>2019-07-12</td>\n",
       "      <td>RUR</td>\n",
       "      <td>e1e424ceb5e4</td>\n",
       "      <td>full</td>\n",
       "      <td>noExperience</td>\n",
       "      <td>fullDay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1002</td>\n",
       "      <td>600.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>BYR</td>\n",
       "      <td>943fd4a3770a</td>\n",
       "      <td>full</td>\n",
       "      <td>between1And3</td>\n",
       "      <td>fullDay</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           specializations  is_train  area_id  \\\n",
       "vacancy_id                                                      \n",
       "1           [242, 256, 302, 324, 358, 440]      True       26   \n",
       "2                                      NaN     False      160   \n",
       "3                                    [211]      True     1002   \n",
       "4                          [389, 412, 437]      True       22   \n",
       "5                                      NaN     False     1002   \n",
       "\n",
       "            compensation_from  compensation_to creation_date currency  \\\n",
       "vacancy_id                                                              \n",
       "1                     22000.0          24000.0    2019-01-24      RUR   \n",
       "2                         NaN              NaN    2019-07-26      NaN   \n",
       "3                         NaN              NaN    2019-04-15      NaN   \n",
       "4                         NaN          36000.0    2019-07-12      RUR   \n",
       "5                       600.0              NaN    2019-01-17      BYR   \n",
       "\n",
       "                employer employment work_experience work_schedule  \n",
       "vacancy_id                                                         \n",
       "1           0ce23382345c       full    between1And3       fullDay  \n",
       "2           b9aa259f8724       full    between1And3       fullDay  \n",
       "3           11ecc72a7a76    project    between1And3       fullDay  \n",
       "4           e1e424ceb5e4       full    noExperience       fullDay  \n",
       "5           943fd4a3770a       full    between1And3       fullDay  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_specializations(s):\n",
    "    res = s[1:-1].split(',')\n",
    "    res = map(int, res)\n",
    "    res = list(res)\n",
    "    # res = np.asarray(res, dtype=int)\n",
    "    return res\n",
    "\n",
    "vacancies_file = os.path.join(WORKDIR, 'data/vacancies_info.csv.gz')\n",
    "\n",
    "if not os.path.isfile(vacancies_file):\n",
    "    # Загружаем специализации для обучения\n",
    "    df_train_ids = pd.read_csv(\n",
    "        os.path.join(WORKDIR, 'train_labels.csv.gz'),\n",
    "        index_col='vacancy_id',\n",
    "        compression='gzip',\n",
    "    )\n",
    "\n",
    "    df_train_ids['specializations'] = df_train_ids['specializations'].map(parse_specializations)\n",
    "    df_train_ids['is_train'] = True\n",
    "\n",
    "    # Загружаем специализации для теста\n",
    "    df_test_ids = pd.read_csv(\n",
    "        os.path.join(WORKDIR, 'test_vacancy_ids.csv.gz'),\n",
    "        index_col='vacancy_id',\n",
    "        compression='gzip',\n",
    "    )\n",
    "\n",
    "    # Объединяем в один датафрейм\n",
    "    df_all_ids = pd.concat([df_train_ids, df_test_ids], axis=0)\n",
    "    df_all_ids['is_train'].fillna(False, inplace=True)\n",
    "    df_all_ids.sort_index(inplace=True)\n",
    "\n",
    "    # Загружаем информацию о каждой из вакансий\n",
    "    df_vacancies_info = pd.read_csv(\n",
    "        os.path.join(WORKDIR, 'vacancies_info.csv.gz'),\n",
    "        index_col='vacancy_id',\n",
    "        compression='gzip',\n",
    "    )\n",
    "\n",
    "    # Объединяем в один датафрейм\n",
    "    df_all_ids = pd.merge(df_all_ids, df_vacancies_info, left_index=True, right_index=True, how='left')\n",
    "\n",
    "    df_all_ids.to_csv(vacancies_file, index=True, compression='gzip')\n",
    "else:\n",
    "    df_all_ids = pd.read_csv(\n",
    "        vacancies_file,\n",
    "        index_col='vacancy_id',\n",
    "        compression='gzip',\n",
    "    )\n",
    "    df_all_ids.loc[df_all_ids['is_train'], 'specializations'] = \\\n",
    "        df_all_ids.loc[df_all_ids['is_train'], 'specializations'].map(parse_specializations)\n",
    "\n",
    "df_all_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "xVYw4hGGCOMo",
    "outputId": "64f93271-d79b-47f7-c7c7-7ee94a8f60c5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>specializations</th>\n",
       "      <th>is_train</th>\n",
       "      <th>area_id</th>\n",
       "      <th>compensation_from</th>\n",
       "      <th>compensation_to</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>currency</th>\n",
       "      <th>employer</th>\n",
       "      <th>employment</th>\n",
       "      <th>work_experience</th>\n",
       "      <th>work_schedule</th>\n",
       "      <th>joined_work</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vacancy_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[242, 256, 302, 324, 358, 440]</td>\n",
       "      <td>True</td>\n",
       "      <td>26</td>\n",
       "      <td>22000.0</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>2019-01-24</td>\n",
       "      <td>RUR</td>\n",
       "      <td>0ce23382345c</td>\n",
       "      <td>full</td>\n",
       "      <td>between1And3</td>\n",
       "      <td>fullDay</td>\n",
       "      <td>full_between1And3_fullDay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-07-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b9aa259f8724</td>\n",
       "      <td>full</td>\n",
       "      <td>between1And3</td>\n",
       "      <td>fullDay</td>\n",
       "      <td>full_between1And3_fullDay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[211]</td>\n",
       "      <td>True</td>\n",
       "      <td>1002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11ecc72a7a76</td>\n",
       "      <td>project</td>\n",
       "      <td>between1And3</td>\n",
       "      <td>fullDay</td>\n",
       "      <td>project_between1And3_fullDay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[389, 412, 437]</td>\n",
       "      <td>True</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36000.0</td>\n",
       "      <td>2019-07-12</td>\n",
       "      <td>RUR</td>\n",
       "      <td>e1e424ceb5e4</td>\n",
       "      <td>full</td>\n",
       "      <td>noExperience</td>\n",
       "      <td>fullDay</td>\n",
       "      <td>full_noExperience_fullDay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1002</td>\n",
       "      <td>600.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>BYR</td>\n",
       "      <td>943fd4a3770a</td>\n",
       "      <td>full</td>\n",
       "      <td>between1And3</td>\n",
       "      <td>fullDay</td>\n",
       "      <td>full_between1And3_fullDay</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           specializations  is_train  area_id  \\\n",
       "vacancy_id                                                      \n",
       "1           [242, 256, 302, 324, 358, 440]      True       26   \n",
       "2                                      NaN     False      160   \n",
       "3                                    [211]      True     1002   \n",
       "4                          [389, 412, 437]      True       22   \n",
       "5                                      NaN     False     1002   \n",
       "\n",
       "            compensation_from  compensation_to creation_date currency  \\\n",
       "vacancy_id                                                              \n",
       "1                     22000.0          24000.0    2019-01-24      RUR   \n",
       "2                         NaN              NaN    2019-07-26      NaN   \n",
       "3                         NaN              NaN    2019-04-15      NaN   \n",
       "4                         NaN          36000.0    2019-07-12      RUR   \n",
       "5                       600.0              NaN    2019-01-17      BYR   \n",
       "\n",
       "                employer employment work_experience work_schedule  \\\n",
       "vacancy_id                                                          \n",
       "1           0ce23382345c       full    between1And3       fullDay   \n",
       "2           b9aa259f8724       full    between1And3       fullDay   \n",
       "3           11ecc72a7a76    project    between1And3       fullDay   \n",
       "4           e1e424ceb5e4       full    noExperience       fullDay   \n",
       "5           943fd4a3770a       full    between1And3       fullDay   \n",
       "\n",
       "                             joined_work  \n",
       "vacancy_id                                \n",
       "1              full_between1And3_fullDay  \n",
       "2              full_between1And3_fullDay  \n",
       "3           project_between1And3_fullDay  \n",
       "4              full_noExperience_fullDay  \n",
       "5              full_between1And3_fullDay  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_ids['joined_work'] = (\n",
    "    df_all_ids['employment'] + '_' +\n",
    "    df_all_ids['work_experience'] + '_' +\n",
    "    df_all_ids['work_schedule']\n",
    ")\n",
    "\n",
    "df_all_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kmK6nPV0nSDz"
   },
   "outputs": [],
   "source": [
    "def make_onehot_csr_matrix(s: pd.Series):\n",
    "    mapping = defaultdict(lambda: len(mapping))\n",
    "\n",
    "    data = np.ones(shape=(s.shape[0], ))\n",
    "    indices = [mapping[k] for k in s]\n",
    "    indptr = np.arange(0, len(data) + 1)\n",
    "\n",
    "    X = sp.csr_matrix((data, indices, indptr), shape=(len(indptr) - 1, len(mapping)))\n",
    "    mapping.default_factory = None\n",
    "    mapping_inv = sorted(mapping, key=lambda e: mapping[e])\n",
    "\n",
    "    return mapping, mapping_inv, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-unk7xstCUVp",
    "outputId": "ee149bb9-9c57-4a85-c64b-9fdaa8cf3ada"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2912650, 345193)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_employer, mapping_employer_inv, features_employer = \\\n",
    "    make_onehot_csr_matrix(df_all_ids['employer'])\n",
    "\n",
    "features_employer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mNFMecu3m6VU"
   },
   "outputs": [],
   "source": [
    "def make_onehot_multiple_csr_matrix(s: pd.Series):\n",
    "    mapping = defaultdict(lambda: len(mapping))\n",
    "\n",
    "    data, indices, indptr = [], [], [0, ]\n",
    "\n",
    "    for row in tqdm(s):\n",
    "        if isinstance(row, list):\n",
    "            row = list(map(lambda e: mapping[e], row))\n",
    "        else:\n",
    "            row = []\n",
    "\n",
    "        data.extend([1] * len(row))\n",
    "        indices.extend(row)\n",
    "        indptr.append(len(data))\n",
    "\n",
    "    X = sp.csr_matrix((data, indices, indptr), shape=(len(indptr) - 1, len(mapping)))\n",
    "    mapping.default_factory = None\n",
    "    mapping_inv = sorted(mapping, key=lambda e: mapping[e])\n",
    "\n",
    "    return mapping, mapping_inv, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "R3Lr1eZYCZKk",
    "outputId": "f8dafdab-adab-4a8c-928a-a95460c00137"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2912650/2912650 [00:03<00:00, 841517.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2912650, 620)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_spec, mapping_spec_inv, y_spec = \\\n",
    "    make_onehot_multiple_csr_matrix(df_all_ids.loc[:, 'specializations'])\n",
    "\n",
    "y_spec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "zYm7HjJkLU9L",
    "outputId": "586b60c5-a7ad-4fb8-9f94-9a6418ab9a5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vacancies-01.json.gz',\n",
       " 'vacancies-02.json.gz',\n",
       " 'vacancies-03.json.gz',\n",
       " 'vacancies-04.json.gz',\n",
       " 'vacancies-05.json.gz',\n",
       " 'vacancies-06.json.gz',\n",
       " 'vacancies-07.json.gz',\n",
       " 'vacancies-08.json.gz',\n",
       " 'vacancies-09.json.gz',\n",
       " 'vacancies-10.json.gz']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vacancies_parts = (f for f in os.listdir(WORKDIR) if f.startswith('vacancies-'))\n",
    "vacancies_parts = sorted(vacancies_parts)\n",
    "vacancies_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cmv0omfcLbLy"
   },
   "outputs": [],
   "source": [
    "def read_vacancies_part(filename):\n",
    "    with gzip.open(filename, mode='r') as f_gz:\n",
    "        records = json.load(f_gz)\n",
    "        records = {int(k): v for k, v in records.items()}\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XJgRAIyLNh-6"
   },
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "\n",
    "ru_morph = MorphAnalyzer()\n",
    "\n",
    "@lru_cache(maxsize=15000)\n",
    "def morph_process(token):\n",
    "    return ru_morph.parse(token)[0].normal_form\n",
    "\n",
    "\n",
    "stop_words = map(morph_process, stopwords.words('russian'))\n",
    "stop_words = stopwords.words('russian') + list(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kzw4Qr2HOdrt",
    "outputId": "3506884d-0777-428a-bd6f-badf3d7bbc4b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "vacancies_it = map(lambda p: os.path.join(WORKDIR, p), tqdm(vacancies_parts))\n",
    "vacancies_it = map(read_vacancies_part, vacancies_it)\n",
    "vacancies_it = ((k, v) for p in vacancies_it for k, v in p.items())\n",
    "\n",
    "index = []\n",
    "\n",
    "def content_names_reader(index):\n",
    "    for vacancy_id, vacancy_info in vacancies_it:\n",
    "        name = re.sub('\\(.*?\\)', '', vacancy_info['name'].lower())\n",
    "        index.append(vacancy_id)\n",
    "        yield name\n",
    "\n",
    "content_names = content_names_reader(index)\n",
    "content_names = tqdm(content_names, position=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ltnZRdzoNlve"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def create_tfidf_vectorizer(**params):\n",
    "    vec = TfidfVectorizer(\n",
    "        stop_words=stop_words,\n",
    "        preprocessor=morph_process,\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=5,\n",
    "        **params\n",
    "    )\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sPvZCxhhOj8a",
    "outputId": "b200713c-9277-47b9-d417-9963eeb72200"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2912650, 139435)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_names_array_file = os.path.join(WORKDIR, 'data/content_names_2.npz')\n",
    "content_names_terms_idfs = os.path.join(WORKDIR, 'data/content_names_2.idf')\n",
    "content_names_vacancies_mapping = os.path.join(WORKDIR, 'data/content_names_2.mapping')\n",
    "\n",
    "if not os.path.isfile(content_names_array_file):\n",
    "    vec = create_tfidf_vectorizer()\n",
    "\n",
    "    # Считаем tfidf-вектора и сохраняем их\n",
    "    features_content_names = vec.fit_transform(content_names)\n",
    "    save_array(features_content_names, content_names_array_file, sparse=True)\n",
    "\n",
    "    # Сохраняем словарик с idf\n",
    "    vocabulary_inv = sorted(vec.vocabulary_, key=lambda e: vec.vocabulary_[e])\n",
    "    with open(content_names_terms_idfs, mode='w', encoding='utf8') as f_data:\n",
    "        for word, idf in zip(vocabulary_inv, vec.idf_):\n",
    "            print(word, \"%.16f\" % idf, sep='\\t', file=f_data)\n",
    "\n",
    "    # Сохраняем порядок вакансий в матрице\n",
    "    with open(content_names_vacancies_mapping, mode='w') as f_data:\n",
    "        print(*index, sep='\\n', file=f_data)\n",
    "else:\n",
    "    # Грузим tfidf-вектора\n",
    "    features_content_names = load_array(content_names_array_file, sparse=True)\n",
    "\n",
    "    # Грузим TfIdfVectorizer\n",
    "    with open(content_names_terms_idfs, mode='r') as f_data:\n",
    "        f_data = map(lambda s: s.rstrip().split('\\t'), f_data)\n",
    "\n",
    "        vocabulary_inv, vocabulary_idf = [], []\n",
    "        for i, (word, idf) in enumerate(f_data):\n",
    "            vocabulary_inv.append(word)\n",
    "            vocabulary_idf.append(float(idf))\n",
    "\n",
    "    vec = create_tfidf_vectorizer(vocabulary=vocabulary_inv)\n",
    "    vec.idf_ = np.asarray(vocabulary_idf, dtype=float)\n",
    "\n",
    "    # Грузим порядок документов\n",
    "    with open(content_names_vacancies_mapping, mode='r') as f_data:\n",
    "        index = list(map(int, f_data))\n",
    "\n",
    "# Убеждаемся, что все правильно\n",
    "assert (np.asarray(index) == df_all_ids.index.values).all()\n",
    "assert features_content_names.shape == (df_all_ids.shape[0], len(vec.idf_))\n",
    "\n",
    "features_content_names.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yuNOFQiXvyYd"
   },
   "outputs": [],
   "source": [
    "def get_best_ranks(ranks: np.ndarray, top: int, axis: int = 0, return_ranks: bool = False):\n",
    "    top_slice = (slice(None), ) * axis + (slice(-top, None), )\n",
    "    inv_slice = (slice(None), ) * axis + (slice(None, None, -1), )\n",
    "\n",
    "    if top < ranks.shape[axis]:\n",
    "        indices = np.argpartition(ranks, -top, axis=axis)[top_slice]\n",
    "        ranks_top = np.take_along_axis(ranks, indices, axis=axis)\n",
    "        indices = np.take_along_axis(indices, ranks_top.argsort(axis=axis)[inv_slice], axis=axis)\n",
    "    else:\n",
    "        indices = np.argsort(ranks, axis=axis)[top_slice]\n",
    "        indices = indices[inv_slice]\n",
    "\n",
    "    result = (indices, )\n",
    "\n",
    "    if return_ranks:\n",
    "        ranks = np.take_along_axis(ranks, indices, axis=axis)\n",
    "        result += (ranks, )\n",
    "\n",
    "    return result if len(result) > 1 else result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yyWEgCtTv4n-"
   },
   "outputs": [],
   "source": [
    "def f1score(y_true, y_pred):\n",
    "    # y_true = set(y_true)\n",
    "    # y_pred = set(y_pred)\n",
    "    \n",
    "    tp = len(y_true & y_pred)\n",
    "    precision = tp / len(y_pred)\n",
    "    recall = tp / len(y_true)\n",
    "    if precision == 0.0 and recall == 0.0:\n",
    "        score = 0.0\n",
    "    else:\n",
    "        score = 2 * precision * recall / (precision + recall)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4FRoThULolw0"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "mask = df_all_ids['is_train'].values.ravel()\n",
    "\n",
    "indices_train, indices_valid = train_test_split(np.where(mask)[0], test_size=0.3, random_state=9872)\n",
    "indices_test = np.where(~mask)[0]\n",
    "\n",
    "y_spec_train = y_spec.copy().tolil()\n",
    "y_spec_train[indices_valid] = 0\n",
    "y_spec_train = y_spec_train.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightfm_predict(model, item_features=None, user_features=None, item_indices=None, user_indices=None):\n",
    "    user_bias, user_embedding = model.get_user_representations(features=user_features)\n",
    "    item_bias, item_embedding = model.get_item_representations(features=item_features)\n",
    "    \n",
    "    if user_indices is not None:\n",
    "        user_bias, user_embedding = user_bias[user_indices], user_embedding[user_indices]\n",
    "    \n",
    "    if item_indices is not None:\n",
    "        item_bias, item_embedding = item_bias[item_indices], item_embedding[item_indices]\n",
    "    \n",
    "    y_pred = np.matmul(user_embedding, item_embedding.T) + item_bias[np.newaxis, :] + user_bias[:, np.newaxis]\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predict(model, indices, top=3, batch_size=2048, return_ranks=False, verbose=True):\n",
    "    y_pred, y_true, ranks_pred_all = [], [], []\n",
    "    \n",
    "    batches = range(0, indices.shape[0], batch_size)\n",
    "    if verbose:\n",
    "        batches = tqdm(batches, position=0, leave=False)\n",
    "    \n",
    "    for i in batches:\n",
    "        j = min(i + batch_size, indices.shape[0])\n",
    "\n",
    "        y_batch_pred = model.predict_batch(\n",
    "            user_indices=indices[i:j],\n",
    "        )\n",
    "        \n",
    "        res = get_best_ranks(y_batch_pred, top=top, axis=1, return_ranks=return_ranks)\n",
    "        if return_ranks:\n",
    "            indices_pred, ranks_pred = res\n",
    "        else:\n",
    "            indices_pred, ranks_pred = res, None\n",
    "            \n",
    "        y_pred.append(indices_pred)\n",
    "        ranks_pred_all.append(ranks_pred)\n",
    "            \n",
    "    y_true = y_spec[indices]\n",
    "    y_true = y_true if y_true.nnz else None\n",
    "    y_pred = np.vstack(y_pred)\n",
    "    \n",
    "    res = (y_true, y_pred, )\n",
    "    \n",
    "    if return_ranks:\n",
    "        ranks_pred = np.vstack(ranks_pred_all)\n",
    "        res += (ranks_pred, )\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, indices, top=3):\n",
    "    y_true, y_pred = make_predict(model, indices, top=top, return_ranks=False)\n",
    "    \n",
    "    y_true = list(map(set, y_true.tolil().rows))\n",
    "    y_pred = list(map(set, y_pred))\n",
    "\n",
    "    assert len(y_true) == len(y_pred)\n",
    "\n",
    "    scores = tqdm(zip(y_true, y_pred), total=len(y_true), position=0, leave=False)\n",
    "    scores = [f1score(*pair) for pair in scores]\n",
    "    scores = np.asarray(scores)\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "hDQuouiwfCED",
    "outputId": "14480413-fdd7-4d90-d85b-18cb07ba6a34"
   },
   "outputs": [],
   "source": [
    "from lightfm import LightFM as BasicLightFM\n",
    "\n",
    "\n",
    "class LightFM(BasicLightFM):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def eval(self, item_features=None, user_features=None):\n",
    "        self._user_bias, self._user_embedding = self.get_user_representations(features=user_features)\n",
    "        self._item_bias, self._item_embedding = self.get_item_representations(features=item_features)\n",
    "        \n",
    "    def predict_batch(self, item_indices=None, user_indices=None, use_item_bias=True, use_user_bias=True):\n",
    "        user_bias, user_embedding = self._user_bias, self._user_embedding\n",
    "        item_bias, item_embedding = self._item_bias, self._item_embedding\n",
    "        \n",
    "        if user_indices is not None:\n",
    "            user_bias, user_embedding = user_bias[user_indices], user_embedding[user_indices]\n",
    "\n",
    "        if item_indices is not None:\n",
    "            item_bias, item_embedding = item_bias[item_indices], item_embedding[item_indices]\n",
    "\n",
    "        y_pred = np.matmul(user_embedding, item_embedding.T)\n",
    "        if use_item_bias:\n",
    "            y_pred += item_bias[np.newaxis, :]\n",
    "        if use_user_bias:\n",
    "            y_pred += user_bias[:, np.newaxis]\n",
    "        return y_pred\n",
    "\n",
    "    \n",
    "model = LightFM(\n",
    "    no_components=200, \n",
    "    loss='warp', \n",
    "    learning_rate=0.03, \n",
    "    max_sampled=400, \n",
    "    random_state=1,\n",
    "    user_alpha=1e-05,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01; f-measure = 0.488451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02; f-measure = 0.496224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03; f-measure = 0.498689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04; f-measure = 0.500210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05; f-measure = 0.500713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06; f-measure = 0.500761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07; f-measure = 0.500418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08; f-measure = 0.500254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09; f-measure = 0.499656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10; f-measure = 0.499450\n",
      "CPU times: user 2h 46min, sys: 41min 47s, total: 3h 27min 48s\n",
      "Wall time: 10min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "score_best = 0\n",
    "\n",
    "for epoch_i in range(10):\n",
    "    model.fit_partial(\n",
    "        y_spec_train,\n",
    "        epochs=1,\n",
    "        num_threads=32,\n",
    "        user_features=features_content_names,\n",
    "    )\n",
    "    \n",
    "    model.eval(user_features=features_content_names)\n",
    "    score = validate(model, indices_valid, top=3)\n",
    "    print('Epoch: {:02d}; f-measure = {:.6f}'.format(epoch_i + 1, score))\n",
    "    \n",
    "    if score > score_best:\n",
    "        model_name = os.path.join(WORKDIR, 'models/lightfm_best.bin')\n",
    "        joblib.dump(model, open(model_name, 'wb'))\n",
    "        score_best = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = os.path.join(WORKDIR, 'models/lightfm_best.bin')\n",
    "model = joblib.load(open(model_name, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    }
   ],
   "source": [
    "_, indices_valid, ranks_valid = make_predict(model, indices_valid, top=15, return_ranks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(436898, 15)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_valid_file = os.path.join(WORKDIR, 'data/lightfm-valid.indices.npz')\n",
    "ranks_valid_file = os.path.join(WORKDIR, 'data/lightfm-valid.ranks.npz')\n",
    "\n",
    "save_array(ranks_valid, ranks_valid_file, sparse=False)\n",
    "save_array(indices_valid, indices_valid_file, sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    }
   ],
   "source": [
    "_, indices_test, ranks_test = make_predict(model, indices_test, top=15, return_ranks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1456325, 15)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_test_file = os.path.join(WORKDIR, 'data/lightfm-test.indices.npz')\n",
    "ranks_test_file = os.path.join(WORKDIR, 'data/lightfm-test.ranks.npz')\n",
    "\n",
    "save_array(ranks_test, ranks_test_file, sparse=False)\n",
    "save_array(indices_test, indices_test_file, sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.6 s, sys: 894 ms, total: 17.5 s\n",
      "Wall time: 17.7 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>specializations</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vacancy_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[82, 211, 278]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[494, 497, 541]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[495, 556, 588]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[83, 287, 387]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[205, 264, 429]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            specializations\n",
       "vacancy_id                 \n",
       "2            [82, 211, 278]\n",
       "5           [494, 497, 541]\n",
       "7           [495, 556, 588]\n",
       "8            [83, 287, 387]\n",
       "10          [205, 264, 429]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def convert_specializations(s):\n",
    "    s = set(s)\n",
    "    s = map(lambda e: mapping_spec_inv[e], s)\n",
    "    s = sorted(s)\n",
    "    # s = np.asarray(s, dtype=int)\n",
    "    return s\n",
    "\n",
    "use_smart = False\n",
    "\n",
    "y_pred_all = []\n",
    "\n",
    "if use_smart:\n",
    "    mask = ranks_test > threshold_best\n",
    "    mask[:, 0] = True\n",
    "\n",
    "    indices_pred = np.where(mask, indices_test, -1)\n",
    "    indices_pred = map(lambda e: filter(lambda x: x >= 0, e), indices_pred)\n",
    "else:\n",
    "    indices_pred = indices_test[:,:3]\n",
    "\n",
    "y_pred_all = list(map(convert_specializations, indices_pred))\n",
    "\n",
    "df_submission = df_all_ids.loc[~df_all_ids['is_train'], ['specializations']]\n",
    "df_submission['specializations'] = y_pred_all\n",
    "\n",
    "submission_id = 21\n",
    "submission_file = os.path.join(WORKDIR, 'submission_{:03d}.csv.gz').format(submission_id)\n",
    "\n",
    "df_submission.to_csv(submission_file, index=True, compression='gzip')\n",
    "df_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zSBKY-W9GQr6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled3-knn-2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
